{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple parameters of this augmentation\n",
    "OUTPUT_PATH = 'Augmented Datasets/'\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_PATH):   \n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "\n",
    "import torch\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anon\\miniconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "fever_plus = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original hyp:  Roman Atwood is a content creator.\n",
      "Updated hyp:  Roman Atwood is a content creator .\n",
      "Original hyp:  The Boston Celtics play their home games at TD Garden.\n",
      "Updated hyp:  The Boston Celtics play their home games at TD Garden .\n",
      "Original hyp:  There is a movie called The Hunger Games.\n",
      "Updated hyp:  There is a movie called The Hunger Games .\n",
      "Original hyp:  Ryan Seacrest is a person.\n",
      "Updated hyp:  Ryan Seacrest is a person .\n",
      "Original hyp:  Stranger than Fiction is a film.\n",
      "Updated hyp:  Stranger than Fiction is a film .\n",
      "Original hyp:  Selena recorded music.\n",
      "Updated hyp:  Selena recorded music .\n",
      "Original hyp:  Selena recorded music.\n",
      "Updated hyp:  Selena recorded music .\n",
      "Original hyp:  Selena recorded music.\n",
      "Updated hyp:  Selena recorded music .\n",
      "Original hyp:  Selena recorded music.\n",
      "Updated hyp:  Selena recorded music .\n",
      "Original hyp:  John Wick: Chapter 2 was theatrically released in the Oregon.\n",
      "Updated hyp:  John Wick : Chapter 2 was theatrically released in the Oregon .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_synonym(word, pos='n'):\n",
    "    try:\n",
    "        word_sense = wn.synset(word[\"nltkSynset\"])\n",
    "        lemmas = word_sense.lemmas()\n",
    "        \n",
    "        # Find a synonym that is not the original word\n",
    "        for lemma in lemmas:\n",
    "            if lemma.name() != word[\"text\"]:  # Compare with original word text\n",
    "                # print(f\"Replacing '{word['text']}' with '{lemma.name()}'\")\n",
    "                return lemma.name()  # Return the synonym\n",
    "    except:\n",
    "        return word[\"text\"]  # Return the original word if any error occurs\n",
    "    \n",
    "    return word[\"text\"]  # Return the original word if no synonym is found\n",
    "\n",
    "\n",
    "def replace_nouns_with_synonyms(entry, pos='NOUN'):\n",
    "    updated_hypothesis = []  # Collect updated words\n",
    "    for word in entry[\"wsd\"][\"hypothesis\"]:\n",
    "        if word[\"pos\"] == pos:  # Check if the word is a noun\n",
    "            synonym = get_synonym(word, pos='n')\n",
    "            updated_hypothesis.append(synonym)  # Replace with the synonym\n",
    "        else:\n",
    "            updated_hypothesis.append(word[\"text\"])  # Keep the original word\n",
    "    # Join the updated words into a single string hypothesis\n",
    "    entry[\"hypothesis\"] = ' '.join(updated_hypothesis)\n",
    "    return entry\n",
    "\n",
    "\n",
    "\n",
    "# Apply the synonym replacement and print updated hypotheses (test)\n",
    "for i in range(10):\n",
    "    print(\"Original hyp: \", fever_plus['train'][i][\"hypothesis\"])\n",
    "    updated_entry = replace_nouns_with_synonyms(fever_plus[\"train\"][i], pos='NOUN')\n",
    "    print(\"Updated hyp: \", updated_entry['hypothesis'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Dataset: 100%|██████████| 51086/51086 [03:37<00:00, 234.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Dataset: 100%|██████████| 2288/2288 [00:09<00:00, 232.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Dataset: 100%|██████████| 2287/2287 [00:09<00:00, 234.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#############################\n",
    "# FULL AUGMENT LOOP:        #\n",
    "#    SYNONIMS SWAPPER       #\n",
    "#############################\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Synonims Swapper aumenter function for a single dataset\n",
    "def SYN_augment(dataset):\n",
    "  syn_list = []\n",
    "\n",
    "  for entry in tqdm(dataset, desc=\"Augmenting Dataset\"):\n",
    "      \n",
    "      updated_entry = replace_nouns_with_synonyms(entry, pos='NOUN')\n",
    "\n",
    "      syn_list.append({\n",
    "                      'id' : entry['id'],\n",
    "                      'premise' : entry['premise'], \n",
    "                      'old_hypothesis': entry['hypothesis'], \n",
    "                      'augmented_hypothesis' : updated_entry['hypothesis'], \n",
    "                      'label' : entry['label'], \n",
    "                      'wsd' : entry['wsd'], \n",
    "                      'srl' : entry['srl']\n",
    "                    })  \n",
    "\n",
    "  return syn_list\n",
    "\n",
    "\n",
    "\n",
    "syn_augmented_fever_train = []\n",
    "syn_augmented_fever_validation = []\n",
    "syn_augmented_fever_test = []\n",
    "\n",
    "# WR-Augmented datasets creation\n",
    "syn_augmented_fever_train = pd.DataFrame(SYN_augment(fever_plus['train']))\n",
    "print('train dataset done')\n",
    "syn_augmented_fever_validation = pd.DataFrame(SYN_augment(fever_plus['validation']))\n",
    "print('validation dataset done')\n",
    "syn_augmented_fever_test = pd.DataFrame(SYN_augment(fever_plus['test']))\n",
    "print('test dataset done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output files created\n"
     ]
    }
   ],
   "source": [
    "# Write DataFrames to jsonl files\n",
    "syn_augmented_fever_train.to_json(OUTPUT_PATH + 'fever_train_syn.jsonl', orient='records', lines=True)\n",
    "syn_augmented_fever_validation.to_json(OUTPUT_PATH + 'fever_validation_syn.jsonl', orient='records', lines=True)\n",
    "syn_augmented_fever_test.to_json(OUTPUT_PATH + 'fever_test_syn.jsonl', orient='records', lines=True)\n",
    "print('output files created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to huge dimension of train file (and possible the others) it is needed to compress them\n",
    "import zipfile\n",
    "\n",
    "def compress_file(file_name, zip_name, path):\n",
    "    # Create ZIP file in the specified output path\n",
    "    zip_path = path + zip_name\n",
    "    file_path = path + file_name\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        zipf.write(file_path, arcname=file_name, compress_type=zipfile.ZIP_DEFLATED)\n",
    "\n",
    "\n",
    "# Compress augmented datasets\n",
    "compress_file('fever_train_syn.jsonl', 'fever_train_syn.zip', OUTPUT_PATH)\n",
    "compress_file('fever_validation_syn.jsonl', 'fever_validation_syn.zip', OUTPUT_PATH)\n",
    "compress_file('fever_test_syn.jsonl', 'fever_test_syn.zip', OUTPUT_PATH)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
