{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH  = \"Augmented Datasets/\"\n",
    "FEVER_PLUS_PATH = 'tommasobonomo/sem_augmented_fever_nli'\n",
    "ADVERSARIAL_FEVER_PATH = 'https://huggingface.co/datasets/iperbole/adversarial_fever_nli'\n",
    "\n",
    "# Decide if you want to train DeBerta coupled models or not (in thatcase, just finetune the fc)\n",
    "FREEZE_BACKBONE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification\n",
    "\n",
    "# class TwinnedModel(nn.Module):\n",
    "#     def __init__(self, do_finetuning, model_name1='microsoft/deberta-v3-xsmall', model_name2='microsoft/deberta-v3-xsmall', num_classes=3):\n",
    "#         super(TwinnedModel, self).__init__()\n",
    "\n",
    "#         # Load pre-trained models\n",
    "#         self.model1 = DebertaV2ForSequenceClassification.from_pretrained(model_name1, num_labels=num_classes)\n",
    "#         self.model2 = DebertaV2ForSequenceClassification.from_pretrained(model_name2, num_labels=num_classes)\n",
    "\n",
    "#         # Freeze the models if you don't want to train them further\n",
    "#         for param in self.model1.parameters():\n",
    "#             param.requires_grad = do_finetuning\n",
    "#         for param in self.model2.parameters():\n",
    "#             param.requires_grad = do_finetuning\n",
    "\n",
    "#         # Define a fully connected layer to combine the hidden states\n",
    "#         combined_hidden_size = self.model1.config.hidden_size + self.model2.config.hidden_size\n",
    "#         self.fc = nn.Linear(combined_hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "#         # Get outputs from both models (hidden states instead of logits)\n",
    "#         outputs1 = self.model1(input_ids1, attention_mask=attention_mask1, output_hidden_states=True)\n",
    "#         outputs2 = self.model2(input_ids2, attention_mask=attention_mask2, output_hidden_states=True)\n",
    "\n",
    "#         # Get the last hidden state (hidden_states[-1])\n",
    "#         hidden_state1 = outputs1.hidden_states[-1][:, 0, :]  # [CLS] token representation\n",
    "#         hidden_state2 = outputs2.hidden_states[-1][:, 0, :]  # [CLS] token representation\n",
    "\n",
    "#         # Concatenate the hidden states\n",
    "#         combined_hidden = torch.cat((hidden_state1, hidden_state2), dim=1)\n",
    "\n",
    "#         # Pass the combined hidden states through the fully connected layer\n",
    "#         logits = self.fc(combined_hidden)\n",
    "\n",
    "#         return logits\n",
    "\n",
    "# # Example usage\n",
    "# tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-xsmall')\n",
    "\n",
    "# # Define sample input sentences\n",
    "# premise = \"A man is eating food.\"\n",
    "# hypothesis = \"The man is having a meal.\"\n",
    "\n",
    "# # Tokenize the input sentences for both models\n",
    "# inputs1 = tokenizer(premise, hypothesis, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "# inputs2 = tokenizer(hypothesis, premise, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = TwinnedModel(do_finetuning=True)\n",
    "\n",
    "# # Pass the inputs through the model\n",
    "# logits = model(inputs1['input_ids'], inputs1['attention_mask'], inputs2['input_ids'], inputs2['attention_mask'])\n",
    "\n",
    "# # Output logits for each class (entailment, contradiction, neutral)\n",
    "# print(\"Logits:\", logits)\n",
    "\n",
    "# # Get the predicted class by applying torch.argmax\n",
    "# predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# # Define the mapping of index to label\n",
    "# label_map = {0: \"entailment\", 1: \"contradiction\", 2: \"neutral\"}\n",
    "\n",
    "# # Get the corresponding label\n",
    "# predicted_label = label_map[predicted_class]\n",
    "\n",
    "# print(f\"Predicted class: {predicted_class} : {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Datasets/fever_test_wr extracted to Augmented Datasets/\n"
     ]
    }
   ],
   "source": [
    "# Ensure all jsonl files for augmented datasets are here\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Datasets needed:\n",
    "file_paths = [\n",
    "    DATASET_PATH+\"fever_train_wr\",\n",
    "    DATASET_PATH+\"fever_validation_wr\",\n",
    "    DATASET_PATH+\"fever_test_wr\",\n",
    "    DATASET_PATH+\"fever_train_syn\",\n",
    "    DATASET_PATH+\"fever_validation_syn\",\n",
    "    DATASET_PATH+\"fever_test_syn\"\n",
    "]\n",
    "\n",
    "\n",
    "for path in file_paths:\n",
    "    if not os.path.exists(path+'.jsonl'):\n",
    "        # Unzip if not exist already\n",
    "        with zipfile.ZipFile(path+'.zip', 'r') as zip:\n",
    "            zip.extractall(DATASET_PATH)\n",
    "            print(f\"{path} extracted to {DATASET_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and assemble datasets\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load .jsonl dataset\n",
    "def load_jsonl_datasets(premise_file_path, hypothesis_file_path):\n",
    "    #data = {'premise' [], 'hypothesis' [], 'label' []}\n",
    "    premises = []\n",
    "    hypotheses = []\n",
    "    labels = []\n",
    "\n",
    "    # Load premise and labels\n",
    "    with open(premise_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line.strip())\n",
    "            premises.append(entry['augmented_premise'])\n",
    "            labels.append(entry['label'])\n",
    "\n",
    "    # Load hypothesis\n",
    "    with open(hypothesis_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line.strip())\n",
    "            hypotheses.append(entry['augmented_hypothesis'])\n",
    "\n",
    "    # wr_data = pd.read_json(premise_file_path, lines=True)\n",
    "    # syn_data = pd.read_json(hypothesis_file_path, lines=True)\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    data = {'premise': premises, 'hypothesis': hypotheses, 'label': labels}\n",
    "    #data = {'premise' wr_data['augmented_premise'], 'hypothesis' syn_data['augmented_hypothesis'], 'label' syn_data['label']}\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "\n",
    "# Load datasets from jsonl files\n",
    "train_data = load_jsonl_datasets(DATASET_PATH+'fever_train_wr.jsonl', DATASET_PATH+'fever_train_syn.jsonl')\n",
    "val_data = load_jsonl_datasets(DATASET_PATH+'fever_validation_wr.jsonl', DATASET_PATH+'fever_validation_syn.jsonl')\n",
    "test_data = load_jsonl_datasets(DATASET_PATH+'fever_test_wr.jsonl', DATASET_PATH+'fever_test_syn.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# with open(DATASET_PATH+'fever_train_wr.jsonl', 'r', encoding='utf-8') as file:\n",
    "#     for line in file:\n",
    "#         data.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "\n",
    "# Load the dataset\n",
    "fever_plus = load_dataset(FEVER_PLUS_PATH)\n",
    "\n",
    "# Map the required fields and convert to Dataset objects\n",
    "original_train_data = fever_plus['train'].map(lambda entry: {field: entry[field] for field in ['premise', 'hypothesis', 'label']})\n",
    "original_val_data = fever_plus['validation'].map(lambda entry: {field: entry[field] for field in ['premise', 'hypothesis', 'label']})\n",
    "\n",
    "# Combine datasets\n",
    "combined_train_data = concatenate_datasets([original_train_data, train_data])\n",
    "combined_val_data = concatenate_datasets([original_val_data, val_data])\n",
    "\n",
    "# Load other test datasets\n",
    "original_test_data = fever_plus['test']\n",
    "#adv_test_data = load_dataset(ADVERSARIAL_FEVER_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/51086 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '150448', 'premise': \"Roman Atwood . He is best known for his vlogs , where he posts updates about his life on a daily basis . His vlogging channel , `` RomanAtwoodVlogs '' , has a total of 3.3 billion views and 11.9 million subscribers . He also has another YouTube channel called `` RomanAtwood '' , where he posts pranks .\", 'hypothesis': 'Roman Atwood is a content creator .', 'augmented_premise': {'hypothesis': 'Roman Atwood is a content creator .', 'id': '150448', 'label': 'ENTAILMENT', 'premise': \"Roman Atwood . He is best known for his vlogs , where he posts updates about his life on a daily basis . His vlogging channel , `` RomanAtwoodVlogs '' , has a total of 3.3 billion views and 11.9 million subscribers . He also has another YouTube channel called `` RomanAtwood '' , where he posts pranks .\", 'srl': {'hypothesis': {'annotations': [{'englishPropbank': {'frameName': 'be.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [0, 2]}, {'role': 'ARG2', 'score': 1.0, 'span': [3, 6]}]}, 'tokenIndex': 2, 'verbatlas': {'frameName': 'COPULA', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [0, 2]}, {'role': 'Attribute', 'score': 1.0, 'span': [3, 6]}]}}], 'tokens': [{'index': 0, 'rawText': 'Roman'}, {'index': 1, 'rawText': 'Atwood'}, {'index': 2, 'rawText': 'is'}, {'index': 3, 'rawText': 'a'}, {'index': 4, 'rawText': 'content'}, {'index': 5, 'rawText': 'creator'}, {'index': 6, 'rawText': '.'}]}, 'premise': {'annotations': [{'englishPropbank': {'frameName': 'be.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [3, 4]}, {'role': 'ARG2', 'score': 1.0, 'span': [5, 22]}]}, 'tokenIndex': 4, 'verbatlas': {'frameName': 'COPULA', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [3, 4]}, {'role': 'Attribute', 'score': 1.0, 'span': [5, 22]}]}}, {'englishPropbank': {'frameName': 'know.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [3, 4]}, {'role': 'ARGM-MNR', 'score': 1.0, 'span': [5, 6]}, {'role': 'ARG2', 'score': 1.0, 'span': [7, 22]}]}, 'tokenIndex': 6, 'verbatlas': {'frameName': 'KNOW', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [3, 4]}, {'role': 'Attribute', 'score': 1.0, 'span': [5, 6]}, {'role': 'Topic', 'score': 1.0, 'span': [7, 22]}]}}, {'englishPropbank': {'frameName': 'post.01', 'roles': [{'role': 'ARGM-LOC', 'score': 1.0, 'span': [8, 10]}, {'role': 'R-ARGM-LOC', 'score': 1.0, 'span': [11, 12]}, {'role': 'ARG0', 'score': 1.0, 'span': [12, 13]}, {'role': 'ARG1', 'score': 1.0, 'span': [14, 18]}, {'role': 'ARGM-TMP', 'score': 1.0, 'span': [18, 22]}]}, 'tokenIndex': 13, 'verbatlas': {'frameName': 'RECORD', 'roles': [{'role': 'Location', 'score': 1.0, 'span': [8, 10]}, {'role': 'Agent', 'score': 1.0, 'span': [12, 13]}, {'role': 'Theme', 'score': 1.0, 'span': [14, 18]}, {'role': 'Time', 'score': 1.0, 'span': [18, 22]}]}}, {'englishPropbank': {'frameName': 'have.03', 'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [23, 32]}, {'role': 'ARG1', 'score': 1.0, 'span': [33, 43]}]}, 'tokenIndex': 32, 'verbatlas': {'frameName': 'EXIST-WITH-FEATURE', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [23, 32]}, {'role': 'Attribute', 'score': 1.0, 'span': [33, 43]}]}}, {'englishPropbank': {'frameName': 'have.03', 'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [44, 45]}, {'role': 'ARGM-DIS', 'score': 1.0, 'span': [45, 46]}, {'role': 'ARG1', 'score': 1.0, 'span': [47, 60]}]}, 'tokenIndex': 46, 'verbatlas': {'frameName': 'EXIST-WITH-FEATURE', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [44, 45]}, {'role': 'Attribute', 'score': 1.0, 'span': [47, 60]}]}}, {'englishPropbank': {'frameName': 'call.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [47, 50]}, {'role': 'ARG2', 'score': 1.0, 'span': [51, 55]}]}, 'tokenIndex': 50, 'verbatlas': {'frameName': 'NAME', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [47, 50]}, {'role': 'Attribute', 'score': 1.0, 'span': [51, 55]}]}}, {'englishPropbank': {'frameName': 'post.01', 'roles': [{'role': 'ARGM-LOC', 'score': 1.0, 'span': [47, 55]}, {'role': 'R-ARGM-LOC', 'score': 1.0, 'span': [56, 57]}, {'role': 'ARG0', 'score': 1.0, 'span': [57, 58]}, {'role': 'ARG1', 'score': 1.0, 'span': [59, 60]}]}, 'tokenIndex': 58, 'verbatlas': {'frameName': 'RECORD', 'roles': [{'role': 'Location', 'score': 1.0, 'span': [47, 55]}, {'role': 'Agent', 'score': 1.0, 'span': [57, 58]}, {'role': 'Theme', 'score': 1.0, 'span': [59, 60]}]}}], 'tokens': [{'index': 0, 'rawText': 'Roman'}, {'index': 1, 'rawText': 'Atwood'}, {'index': 2, 'rawText': '.'}, {'index': 3, 'rawText': 'He'}, {'index': 4, 'rawText': 'is'}, {'index': 5, 'rawText': 'best'}, {'index': 6, 'rawText': 'known'}, {'index': 7, 'rawText': 'for'}, {'index': 8, 'rawText': 'his'}, {'index': 9, 'rawText': 'vlogs'}, {'index': 10, 'rawText': ','}, {'index': 11, 'rawText': 'where'}, {'index': 12, 'rawText': 'he'}, {'index': 13, 'rawText': 'posts'}, {'index': 14, 'rawText': 'updates'}, {'index': 15, 'rawText': 'about'}, {'index': 16, 'rawText': 'his'}, {'index': 17, 'rawText': 'life'}, {'index': 18, 'rawText': 'on'}, {'index': 19, 'rawText': 'a'}, {'index': 20, 'rawText': 'daily'}, {'index': 21, 'rawText': 'basis'}, {'index': 22, 'rawText': '.'}, {'index': 23, 'rawText': 'His'}, {'index': 24, 'rawText': 'vlogging'}, {'index': 25, 'rawText': 'channel'}, {'index': 26, 'rawText': ','}, {'index': 27, 'rawText': '`'}, {'index': 28, 'rawText': '`'}, {'index': 29, 'rawText': 'RomanAtwoodVlogs'}, {'index': 30, 'rawText': \"''\"}, {'index': 31, 'rawText': ','}, {'index': 32, 'rawText': 'has'}, {'index': 33, 'rawText': 'a'}, {'index': 34, 'rawText': 'total'}, {'index': 35, 'rawText': 'of'}, {'index': 36, 'rawText': '3.3'}, {'index': 37, 'rawText': 'billion'}, {'index': 38, 'rawText': 'views'}, {'index': 39, 'rawText': 'and'}, {'index': 40, 'rawText': '11.9'}, {'index': 41, 'rawText': 'million'}, {'index': 42, 'rawText': 'subscribers'}, {'index': 43, 'rawText': '.'}, {'index': 44, 'rawText': 'He'}, {'index': 45, 'rawText': 'also'}, {'index': 46, 'rawText': 'has'}, {'index': 47, 'rawText': 'another'}, {'index': 48, 'rawText': 'YouTube'}, {'index': 49, 'rawText': 'channel'}, {'index': 50, 'rawText': 'called'}, {'index': 51, 'rawText': '`'}, {'index': 52, 'rawText': '`'}, {'index': 53, 'rawText': 'RomanAtwood'}, {'index': 54, 'rawText': \"''\"}, {'index': 55, 'rawText': ','}, {'index': 56, 'rawText': 'where'}, {'index': 57, 'rawText': 'he'}, {'index': 58, 'rawText': 'posts'}, {'index': 59, 'rawText': 'pranks'}, {'index': 60, 'rawText': '.'}]}}, 'wsd': {'hypothesis': [{'bnSynsetId': 'O', 'index': 0, 'lemma': 'Roman', 'nltkSynset': 'O', 'pos': 'PROPN', 'text': 'Roman', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 1, 'lemma': 'Atwood', 'nltkSynset': 'O', 'pos': 'PROPN', 'text': 'Atwood', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 2, 'lemma': 'be', 'nltkSynset': 'O', 'pos': 'AUX', 'text': 'is', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 3, 'lemma': 'a', 'nltkSynset': 'O', 'pos': 'DET', 'text': 'a', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00100364a', 'index': 4, 'lemma': 'content', 'nltkSynset': 'contented.a.01', 'pos': 'ADJ', 'text': 'content', 'wnSynsetOffset': '588797a'}, {'bnSynsetId': 'bn:00023660n', 'index': 5, 'lemma': 'creator', 'nltkSynset': 'creator.n.02', 'pos': 'NOUN', 'text': 'creator', 'wnSynsetOffset': '9614315n'}, {'bnSynsetId': 'O', 'index': 6, 'lemma': '.', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '.', 'wnSynsetOffset': 'O'}], 'premise': [{'bnSynsetId': 'bn:00109913a', 'index': 0, 'lemma': 'roman', 'nltkSynset': 'roman.a.01', 'pos': 'ADJ', 'text': 'Roman', 'wnSynsetOffset': '2921569a'}, {'bnSynsetId': 'O', 'index': 1, 'lemma': 'Atwood', 'nltkSynset': 'O', 'pos': 'PROPN', 'text': 'Atwood', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 2, 'lemma': '.', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '.', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 3, 'lemma': 'he', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'He', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 4, 'lemma': 'be', 'nltkSynset': 'O', 'pos': 'AUX', 'text': 'is', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00117603r', 'index': 5, 'lemma': 'well', 'nltkSynset': 'well.r.02', 'pos': 'ADV', 'text': 'best', 'wnSynsetOffset': '12779r'}, {'bnSynsetId': 'bn:00090143v', 'index': 6, 'lemma': 'know', 'nltkSynset': 'know.v.04', 'pos': 'VERB', 'text': 'known', 'wnSynsetOffset': '594337v'}, {'bnSynsetId': 'O', 'index': 7, 'lemma': 'for', 'nltkSynset': 'O', 'pos': 'ADP', 'text': 'for', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 8, 'lemma': 'his', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'his', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00028604n', 'index': 9, 'lemma': 'vlog', 'nltkSynset': 'play.n.01', 'pos': 'NOUN', 'text': 'vlogs', 'wnSynsetOffset': '7007945n'}, {'bnSynsetId': 'O', 'index': 10, 'lemma': ',', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': ',', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 11, 'lemma': 'where', 'nltkSynset': 'O', 'pos': 'SCONJ', 'text': 'where', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 12, 'lemma': 'he', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'he', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00091876v', 'index': 13, 'lemma': 'post', 'nltkSynset': 'post.v.02', 'pos': 'VERB', 'text': 'posts', 'wnSynsetOffset': '991683v'}, {'bnSynsetId': 'bn:00079238n', 'index': 14, 'lemma': 'update', 'nltkSynset': 'update.n.01', 'pos': 'NOUN', 'text': 'updates', 'wnSynsetOffset': '6643303n'}, {'bnSynsetId': 'O', 'index': 15, 'lemma': 'about', 'nltkSynset': 'O', 'pos': 'ADP', 'text': 'about', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 16, 'lemma': 'his', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'his', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00051045n', 'index': 17, 'lemma': 'life', 'nltkSynset': 'life.n.01', 'pos': 'NOUN', 'text': 'life', 'wnSynsetOffset': '13963192n'}, {'bnSynsetId': 'O', 'index': 18, 'lemma': 'on', 'nltkSynset': 'O', 'pos': 'ADP', 'text': 'on', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 19, 'lemma': 'a', 'nltkSynset': 'O', 'pos': 'DET', 'text': 'a', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00100875a', 'index': 20, 'lemma': 'daily', 'nltkSynset': 'daily.s.01', 'pos': 'ADJ', 'text': 'daily', 'wnSynsetOffset': '1968165a'}, {'bnSynsetId': 'bn:00008870n', 'index': 21, 'lemma': 'basis', 'nltkSynset': 'footing.n.02', 'pos': 'NOUN', 'text': 'basis', 'wnSynsetOffset': '13790912n'}, {'bnSynsetId': 'O', 'index': 22, 'lemma': '.', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '.', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 23, 'lemma': 'his', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'His', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00090000v', 'index': 24, 'lemma': 'vlogge', 'nltkSynset': 'judge.v.01', 'pos': 'VERB', 'text': 'vlogging', 'wnSynsetOffset': '672277v'}, {'bnSynsetId': 'bn:00017686n', 'index': 25, 'lemma': 'channel', 'nltkSynset': 'channel.n.07', 'pos': 'NOUN', 'text': 'channel', 'wnSynsetOffset': '3006398n'}, {'bnSynsetId': 'O', 'index': 26, 'lemma': ',', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': ',', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 27, 'lemma': '`', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '`', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 28, 'lemma': '`', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '`', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 29, 'lemma': 'romanatwoodvlogs', 'nltkSynset': 'O', 'pos': 'X', 'text': 'RomanAtwoodVlogs', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 30, 'lemma': \"''\", 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': \"''\", 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 31, 'lemma': ',', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': ',', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 32, 'lemma': 'have', 'nltkSynset': 'O', 'pos': 'AUX', 'text': 'has', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 33, 'lemma': 'a', 'nltkSynset': 'O', 'pos': 'DET', 'text': 'a', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00002008n', 'index': 34, 'lemma': 'total', 'nltkSynset': 'sum.n.05', 'pos': 'NOUN', 'text': 'total', 'wnSynsetOffset': '4353803n'}, {'bnSynsetId': 'O', 'index': 35, 'lemma': 'of', 'nltkSynset': 'O', 'pos': 'ADP', 'text': 'of', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 36, 'lemma': '3.3', 'nltkSynset': 'O', 'pos': 'NUM', 'text': '3.3', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 37, 'lemma': 'billion', 'nltkSynset': 'O', 'pos': 'NUM', 'text': 'billion', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00071511n', 'index': 38, 'lemma': 'view', 'nltkSynset': 'view.n.03', 'pos': 'NOUN', 'text': 'views', 'wnSynsetOffset': '881649n'}, {'bnSynsetId': 'O', 'index': 39, 'lemma': 'and', 'nltkSynset': 'O', 'pos': 'CCONJ', 'text': 'and', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 40, 'lemma': '11.9', 'nltkSynset': 'O', 'pos': 'NUM', 'text': '11.9', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 41, 'lemma': 'million', 'nltkSynset': 'O', 'pos': 'NUM', 'text': 'million', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00066366n', 'index': 42, 'lemma': 'subscriber', 'nltkSynset': 'subscriber.n.02', 'pos': 'NOUN', 'text': 'subscribers', 'wnSynsetOffset': '10670483n'}, {'bnSynsetId': 'O', 'index': 43, 'lemma': '.', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '.', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 44, 'lemma': 'he', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'He', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00114246r', 'index': 45, 'lemma': 'also', 'nltkSynset': 'besides.r.02', 'pos': 'ADV', 'text': 'also', 'wnSynsetOffset': '47534r'}, {'bnSynsetId': 'O', 'index': 46, 'lemma': 'have', 'nltkSynset': 'O', 'pos': 'AUX', 'text': 'has', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 47, 'lemma': 'another', 'nltkSynset': 'O', 'pos': 'DET', 'text': 'another', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 48, 'lemma': 'YouTube', 'nltkSynset': 'O', 'pos': 'PROPN', 'text': 'YouTube', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00015144n', 'index': 49, 'lemma': 'channel', 'nltkSynset': 'duct.n.01', 'pos': 'NOUN', 'text': 'channel', 'wnSynsetOffset': '5250659n'}, {'bnSynsetId': 'bn:00084385v', 'index': 50, 'lemma': 'call', 'nltkSynset': 'name.v.01', 'pos': 'VERB', 'text': 'called', 'wnSynsetOffset': '1028748v'}, {'bnSynsetId': 'O', 'index': 51, 'lemma': '`', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '`', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 52, 'lemma': '`', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '`', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 53, 'lemma': 'RomanAtwood', 'nltkSynset': 'O', 'pos': 'PROPN', 'text': 'RomanAtwood', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 54, 'lemma': \"''\", 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': \"''\", 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 55, 'lemma': ',', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': ',', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 56, 'lemma': 'where', 'nltkSynset': 'O', 'pos': 'SCONJ', 'text': 'where', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 57, 'lemma': 'he', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'he', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00091876v', 'index': 58, 'lemma': 'post', 'nltkSynset': 'post.v.02', 'pos': 'VERB', 'text': 'posts', 'wnSynsetOffset': '991683v'}, {'bnSynsetId': 'bn:00004630n', 'index': 59, 'lemma': 'prank', 'nltkSynset': 'antic.n.01', 'pos': 'NOUN', 'text': 'pranks', 'wnSynsetOffset': '427580n'}, {'bnSynsetId': 'O', 'index': 60, 'lemma': '.', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '.', 'wnSynsetOffset': 'O'}]}}, 'label': 'ENTAILMENT', 'wsd': {'hypothesis': [{'bnSynsetId': 'O', 'index': 0, 'lemma': 'Roman', 'nltkSynset': 'O', 'pos': 'PROPN', 'text': 'Roman', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 1, 'lemma': 'Atwood', 'nltkSynset': 'O', 'pos': 'PROPN', 'text': 'Atwood', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 2, 'lemma': 'be', 'nltkSynset': 'O', 'pos': 'AUX', 'text': 'is', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 3, 'lemma': 'a', 'nltkSynset': 'O', 'pos': 'DET', 'text': 'a', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00100364a', 'index': 4, 'lemma': 'content', 'nltkSynset': 'contented.a.01', 'pos': 'ADJ', 'text': 'content', 'wnSynsetOffset': '588797a'}, {'bnSynsetId': 'bn:00023660n', 'index': 5, 'lemma': 'creator', 'nltkSynset': 'creator.n.02', 'pos': 'NOUN', 'text': 'creator', 'wnSynsetOffset': '9614315n'}, {'bnSynsetId': 'O', 'index': 6, 'lemma': '.', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '.', 'wnSynsetOffset': 'O'}], 'premise': [{'bnSynsetId': 'bn:00109913a', 'index': 0, 'lemma': 'roman', 'nltkSynset': 'roman.a.01', 'pos': 'ADJ', 'text': 'Roman', 'wnSynsetOffset': '2921569a'}, {'bnSynsetId': 'O', 'index': 1, 'lemma': 'Atwood', 'nltkSynset': 'O', 'pos': 'PROPN', 'text': 'Atwood', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 2, 'lemma': '.', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '.', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 3, 'lemma': 'he', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'He', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 4, 'lemma': 'be', 'nltkSynset': 'O', 'pos': 'AUX', 'text': 'is', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00117603r', 'index': 5, 'lemma': 'well', 'nltkSynset': 'well.r.02', 'pos': 'ADV', 'text': 'best', 'wnSynsetOffset': '12779r'}, {'bnSynsetId': 'bn:00090143v', 'index': 6, 'lemma': 'know', 'nltkSynset': 'know.v.04', 'pos': 'VERB', 'text': 'known', 'wnSynsetOffset': '594337v'}, {'bnSynsetId': 'O', 'index': 7, 'lemma': 'for', 'nltkSynset': 'O', 'pos': 'ADP', 'text': 'for', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 8, 'lemma': 'his', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'his', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00028604n', 'index': 9, 'lemma': 'vlog', 'nltkSynset': 'play.n.01', 'pos': 'NOUN', 'text': 'vlogs', 'wnSynsetOffset': '7007945n'}, {'bnSynsetId': 'O', 'index': 10, 'lemma': ',', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': ',', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 11, 'lemma': 'where', 'nltkSynset': 'O', 'pos': 'SCONJ', 'text': 'where', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 12, 'lemma': 'he', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'he', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00091876v', 'index': 13, 'lemma': 'post', 'nltkSynset': 'post.v.02', 'pos': 'VERB', 'text': 'posts', 'wnSynsetOffset': '991683v'}, {'bnSynsetId': 'bn:00079238n', 'index': 14, 'lemma': 'update', 'nltkSynset': 'update.n.01', 'pos': 'NOUN', 'text': 'updates', 'wnSynsetOffset': '6643303n'}, {'bnSynsetId': 'O', 'index': 15, 'lemma': 'about', 'nltkSynset': 'O', 'pos': 'ADP', 'text': 'about', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 16, 'lemma': 'his', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'his', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00051045n', 'index': 17, 'lemma': 'life', 'nltkSynset': 'life.n.01', 'pos': 'NOUN', 'text': 'life', 'wnSynsetOffset': '13963192n'}, {'bnSynsetId': 'O', 'index': 18, 'lemma': 'on', 'nltkSynset': 'O', 'pos': 'ADP', 'text': 'on', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 19, 'lemma': 'a', 'nltkSynset': 'O', 'pos': 'DET', 'text': 'a', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00100875a', 'index': 20, 'lemma': 'daily', 'nltkSynset': 'daily.s.01', 'pos': 'ADJ', 'text': 'daily', 'wnSynsetOffset': '1968165a'}, {'bnSynsetId': 'bn:00008870n', 'index': 21, 'lemma': 'basis', 'nltkSynset': 'footing.n.02', 'pos': 'NOUN', 'text': 'basis', 'wnSynsetOffset': '13790912n'}, {'bnSynsetId': 'O', 'index': 22, 'lemma': '.', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '.', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 23, 'lemma': 'his', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'His', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00090000v', 'index': 24, 'lemma': 'vlogge', 'nltkSynset': 'judge.v.01', 'pos': 'VERB', 'text': 'vlogging', 'wnSynsetOffset': '672277v'}, {'bnSynsetId': 'bn:00017686n', 'index': 25, 'lemma': 'channel', 'nltkSynset': 'channel.n.07', 'pos': 'NOUN', 'text': 'channel', 'wnSynsetOffset': '3006398n'}, {'bnSynsetId': 'O', 'index': 26, 'lemma': ',', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': ',', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 27, 'lemma': '`', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '`', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 28, 'lemma': '`', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '`', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 29, 'lemma': 'romanatwoodvlogs', 'nltkSynset': 'O', 'pos': 'X', 'text': 'RomanAtwoodVlogs', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 30, 'lemma': \"''\", 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': \"''\", 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 31, 'lemma': ',', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': ',', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 32, 'lemma': 'have', 'nltkSynset': 'O', 'pos': 'AUX', 'text': 'has', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 33, 'lemma': 'a', 'nltkSynset': 'O', 'pos': 'DET', 'text': 'a', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00002008n', 'index': 34, 'lemma': 'total', 'nltkSynset': 'sum.n.05', 'pos': 'NOUN', 'text': 'total', 'wnSynsetOffset': '4353803n'}, {'bnSynsetId': 'O', 'index': 35, 'lemma': 'of', 'nltkSynset': 'O', 'pos': 'ADP', 'text': 'of', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 36, 'lemma': '3.3', 'nltkSynset': 'O', 'pos': 'NUM', 'text': '3.3', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 37, 'lemma': 'billion', 'nltkSynset': 'O', 'pos': 'NUM', 'text': 'billion', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00071511n', 'index': 38, 'lemma': 'view', 'nltkSynset': 'view.n.03', 'pos': 'NOUN', 'text': 'views', 'wnSynsetOffset': '881649n'}, {'bnSynsetId': 'O', 'index': 39, 'lemma': 'and', 'nltkSynset': 'O', 'pos': 'CCONJ', 'text': 'and', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 40, 'lemma': '11.9', 'nltkSynset': 'O', 'pos': 'NUM', 'text': '11.9', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 41, 'lemma': 'million', 'nltkSynset': 'O', 'pos': 'NUM', 'text': 'million', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00066366n', 'index': 42, 'lemma': 'subscriber', 'nltkSynset': 'subscriber.n.02', 'pos': 'NOUN', 'text': 'subscribers', 'wnSynsetOffset': '10670483n'}, {'bnSynsetId': 'O', 'index': 43, 'lemma': '.', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '.', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 44, 'lemma': 'he', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'He', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00114246r', 'index': 45, 'lemma': 'also', 'nltkSynset': 'besides.r.02', 'pos': 'ADV', 'text': 'also', 'wnSynsetOffset': '47534r'}, {'bnSynsetId': 'O', 'index': 46, 'lemma': 'have', 'nltkSynset': 'O', 'pos': 'AUX', 'text': 'has', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 47, 'lemma': 'another', 'nltkSynset': 'O', 'pos': 'DET', 'text': 'another', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 48, 'lemma': 'YouTube', 'nltkSynset': 'O', 'pos': 'PROPN', 'text': 'YouTube', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00015144n', 'index': 49, 'lemma': 'channel', 'nltkSynset': 'duct.n.01', 'pos': 'NOUN', 'text': 'channel', 'wnSynsetOffset': '5250659n'}, {'bnSynsetId': 'bn:00084385v', 'index': 50, 'lemma': 'call', 'nltkSynset': 'name.v.01', 'pos': 'VERB', 'text': 'called', 'wnSynsetOffset': '1028748v'}, {'bnSynsetId': 'O', 'index': 51, 'lemma': '`', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '`', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 52, 'lemma': '`', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '`', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 53, 'lemma': 'RomanAtwood', 'nltkSynset': 'O', 'pos': 'PROPN', 'text': 'RomanAtwood', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 54, 'lemma': \"''\", 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': \"''\", 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 55, 'lemma': ',', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': ',', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 56, 'lemma': 'where', 'nltkSynset': 'O', 'pos': 'SCONJ', 'text': 'where', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'O', 'index': 57, 'lemma': 'he', 'nltkSynset': 'O', 'pos': 'PRON', 'text': 'he', 'wnSynsetOffset': 'O'}, {'bnSynsetId': 'bn:00091876v', 'index': 58, 'lemma': 'post', 'nltkSynset': 'post.v.02', 'pos': 'VERB', 'text': 'posts', 'wnSynsetOffset': '991683v'}, {'bnSynsetId': 'bn:00004630n', 'index': 59, 'lemma': 'prank', 'nltkSynset': 'antic.n.01', 'pos': 'NOUN', 'text': 'pranks', 'wnSynsetOffset': '427580n'}, {'bnSynsetId': 'O', 'index': 60, 'lemma': '.', 'nltkSynset': 'O', 'pos': 'PUNCT', 'text': '.', 'wnSynsetOffset': 'O'}]}, 'srl': {'hypothesis': {'annotations': [{'englishPropbank': {'frameName': 'be.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [0, 2]}, {'role': 'ARG2', 'score': 1.0, 'span': [3, 6]}]}, 'tokenIndex': 2, 'verbatlas': {'frameName': 'COPULA', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [0, 2]}, {'role': 'Attribute', 'score': 1.0, 'span': [3, 6]}]}}], 'tokens': [{'index': 0, 'rawText': 'Roman'}, {'index': 1, 'rawText': 'Atwood'}, {'index': 2, 'rawText': 'is'}, {'index': 3, 'rawText': 'a'}, {'index': 4, 'rawText': 'content'}, {'index': 5, 'rawText': 'creator'}, {'index': 6, 'rawText': '.'}]}, 'premise': {'annotations': [{'englishPropbank': {'frameName': 'be.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [3, 4]}, {'role': 'ARG2', 'score': 1.0, 'span': [5, 22]}]}, 'tokenIndex': 4, 'verbatlas': {'frameName': 'COPULA', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [3, 4]}, {'role': 'Attribute', 'score': 1.0, 'span': [5, 22]}]}}, {'englishPropbank': {'frameName': 'know.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [3, 4]}, {'role': 'ARGM-MNR', 'score': 1.0, 'span': [5, 6]}, {'role': 'ARG2', 'score': 1.0, 'span': [7, 22]}]}, 'tokenIndex': 6, 'verbatlas': {'frameName': 'KNOW', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [3, 4]}, {'role': 'Attribute', 'score': 1.0, 'span': [5, 6]}, {'role': 'Topic', 'score': 1.0, 'span': [7, 22]}]}}, {'englishPropbank': {'frameName': 'post.01', 'roles': [{'role': 'ARGM-LOC', 'score': 1.0, 'span': [8, 10]}, {'role': 'R-ARGM-LOC', 'score': 1.0, 'span': [11, 12]}, {'role': 'ARG0', 'score': 1.0, 'span': [12, 13]}, {'role': 'ARG1', 'score': 1.0, 'span': [14, 18]}, {'role': 'ARGM-TMP', 'score': 1.0, 'span': [18, 22]}]}, 'tokenIndex': 13, 'verbatlas': {'frameName': 'RECORD', 'roles': [{'role': 'Location', 'score': 1.0, 'span': [8, 10]}, {'role': 'Agent', 'score': 1.0, 'span': [12, 13]}, {'role': 'Theme', 'score': 1.0, 'span': [14, 18]}, {'role': 'Time', 'score': 1.0, 'span': [18, 22]}]}}, {'englishPropbank': {'frameName': 'have.03', 'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [23, 32]}, {'role': 'ARG1', 'score': 1.0, 'span': [33, 43]}]}, 'tokenIndex': 32, 'verbatlas': {'frameName': 'EXIST-WITH-FEATURE', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [23, 32]}, {'role': 'Attribute', 'score': 1.0, 'span': [33, 43]}]}}, {'englishPropbank': {'frameName': 'have.03', 'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [44, 45]}, {'role': 'ARGM-DIS', 'score': 1.0, 'span': [45, 46]}, {'role': 'ARG1', 'score': 1.0, 'span': [47, 60]}]}, 'tokenIndex': 46, 'verbatlas': {'frameName': 'EXIST-WITH-FEATURE', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [44, 45]}, {'role': 'Attribute', 'score': 1.0, 'span': [47, 60]}]}}, {'englishPropbank': {'frameName': 'call.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [47, 50]}, {'role': 'ARG2', 'score': 1.0, 'span': [51, 55]}]}, 'tokenIndex': 50, 'verbatlas': {'frameName': 'NAME', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [47, 50]}, {'role': 'Attribute', 'score': 1.0, 'span': [51, 55]}]}}, {'englishPropbank': {'frameName': 'post.01', 'roles': [{'role': 'ARGM-LOC', 'score': 1.0, 'span': [47, 55]}, {'role': 'R-ARGM-LOC', 'score': 1.0, 'span': [56, 57]}, {'role': 'ARG0', 'score': 1.0, 'span': [57, 58]}, {'role': 'ARG1', 'score': 1.0, 'span': [59, 60]}]}, 'tokenIndex': 58, 'verbatlas': {'frameName': 'RECORD', 'roles': [{'role': 'Location', 'score': 1.0, 'span': [47, 55]}, {'role': 'Agent', 'score': 1.0, 'span': [57, 58]}, {'role': 'Theme', 'score': 1.0, 'span': [59, 60]}]}}], 'tokens': [{'index': 0, 'rawText': 'Roman'}, {'index': 1, 'rawText': 'Atwood'}, {'index': 2, 'rawText': '.'}, {'index': 3, 'rawText': 'He'}, {'index': 4, 'rawText': 'is'}, {'index': 5, 'rawText': 'best'}, {'index': 6, 'rawText': 'known'}, {'index': 7, 'rawText': 'for'}, {'index': 8, 'rawText': 'his'}, {'index': 9, 'rawText': 'vlogs'}, {'index': 10, 'rawText': ','}, {'index': 11, 'rawText': 'where'}, {'index': 12, 'rawText': 'he'}, {'index': 13, 'rawText': 'posts'}, {'index': 14, 'rawText': 'updates'}, {'index': 15, 'rawText': 'about'}, {'index': 16, 'rawText': 'his'}, {'index': 17, 'rawText': 'life'}, {'index': 18, 'rawText': 'on'}, {'index': 19, 'rawText': 'a'}, {'index': 20, 'rawText': 'daily'}, {'index': 21, 'rawText': 'basis'}, {'index': 22, 'rawText': '.'}, {'index': 23, 'rawText': 'His'}, {'index': 24, 'rawText': 'vlogging'}, {'index': 25, 'rawText': 'channel'}, {'index': 26, 'rawText': ','}, {'index': 27, 'rawText': '`'}, {'index': 28, 'rawText': '`'}, {'index': 29, 'rawText': 'RomanAtwoodVlogs'}, {'index': 30, 'rawText': \"''\"}, {'index': 31, 'rawText': ','}, {'index': 32, 'rawText': 'has'}, {'index': 33, 'rawText': 'a'}, {'index': 34, 'rawText': 'total'}, {'index': 35, 'rawText': 'of'}, {'index': 36, 'rawText': '3.3'}, {'index': 37, 'rawText': 'billion'}, {'index': 38, 'rawText': 'views'}, {'index': 39, 'rawText': 'and'}, {'index': 40, 'rawText': '11.9'}, {'index': 41, 'rawText': 'million'}, {'index': 42, 'rawText': 'subscribers'}, {'index': 43, 'rawText': '.'}, {'index': 44, 'rawText': 'He'}, {'index': 45, 'rawText': 'also'}, {'index': 46, 'rawText': 'has'}, {'index': 47, 'rawText': 'another'}, {'index': 48, 'rawText': 'YouTube'}, {'index': 49, 'rawText': 'channel'}, {'index': 50, 'rawText': 'called'}, {'index': 51, 'rawText': '`'}, {'index': 52, 'rawText': '`'}, {'index': 53, 'rawText': 'RomanAtwood'}, {'index': 54, 'rawText': \"''\"}, {'index': 55, 'rawText': ','}, {'index': 56, 'rawText': 'where'}, {'index': 57, 'rawText': 'he'}, {'index': 58, 'rawText': 'posts'}, {'index': 59, 'rawText': 'pranks'}, {'index': 60, 'rawText': '.'}]}}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Provided `function` which is applied to all elements of table returns a variable of type <class 'int'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m test_data \u001b[38;5;241m=\u001b[39m load_jsonl_dataset(DATASET_PATH\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfever_test_syn.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Encode datasets\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m val_data\u001b[38;5;241m.\u001b[39mmap(preprocess_input, fn_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m: tokenizer})\n\u001b[0;32m     45\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mmap(preprocess_input, fn_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m: tokenizer})\n",
      "File \u001b[1;32mc:\\Users\\anon\\miniconda3\\envs\\nlp\\Lib\\site-packages\\datasets\\arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anon\\miniconda3\\envs\\nlp\\Lib\\site-packages\\datasets\\arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    565\u001b[0m }\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anon\\miniconda3\\envs\\nlp\\Lib\\site-packages\\datasets\\arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3157\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3158\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32mc:\\Users\\anon\\miniconda3\\envs\\nlp\\Lib\\site-packages\\datasets\\arrow_dataset.py:3517\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3515\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3516\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[1;32m-> 3517\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[0;32m   3519\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\anon\\miniconda3\\envs\\nlp\\Lib\\site-packages\\datasets\\arrow_dataset.py:3427\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3425\u001b[0m     \u001b[38;5;66;03m# Check if the function returns updated examples\u001b[39;00m\n\u001b[0;32m   3426\u001b[0m     update_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[38;5;241m.\u001b[39mTable, pd\u001b[38;5;241m.\u001b[39mDataFrame))\n\u001b[1;32m-> 3427\u001b[0m     \u001b[43mvalidate_function_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m update_data:\n\u001b[0;32m   3429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Nothing to update, let's move on\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anon\\miniconda3\\envs\\nlp\\Lib\\site-packages\\datasets\\arrow_dataset.py:3368\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.validate_function_output\u001b[1;34m(processed_inputs, indices)\u001b[0m\n\u001b[0;32m   3366\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validate output of the map function.\"\"\"\u001b[39;00m\n\u001b[0;32m   3367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processed_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[38;5;241m.\u001b[39mTable, pd\u001b[38;5;241m.\u001b[39mDataFrame)):\n\u001b[1;32m-> 3368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   3369\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `function` which is applied to all elements of table returns a variable of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(processed_inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3370\u001b[0m     )\n\u001b[0;32m   3371\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indices, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, Mapping):\n\u001b[0;32m   3372\u001b[0m     allowed_batch_return_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, pd\u001b[38;5;241m.\u001b[39mSeries)\n",
      "\u001b[1;31mTypeError\u001b[0m: Provided `function` which is applied to all elements of table returns a variable of type <class 'int'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects."
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import DebertaV2Tokenizer\n",
    "\n",
    "# Function to tokenize and format a single entry\n",
    "def preprocess_input(entry, tokenizer, prem_max_length=256, hyp_max_length=256):\n",
    "    label_map = {'ENTAILMENT': 0, 'CONTRADICTION': 1, 'NEUTRAL': 2}\n",
    "    print(entry)\n",
    "    premise = entry['premise']\n",
    "    hypothesis = entry['hypothesis']\n",
    "    label = entry['label']\n",
    "\n",
    "    \n",
    "    inputs1 = tokenizer(premise, hypothesis, truncation=True, padding='max_length', max_length=prem_max_length)\n",
    "    inputs2 = tokenizer(hypothesis, premise, truncation=True, padding='max_length', max_length=hyp_max_length)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids1\": inputs1['input_ids'],\n",
    "        \"attention_mask1\": inputs1['attention_mask'],\n",
    "        \"input_ids2\": inputs2['input_ids'],\n",
    "        \"attention_mask2\": inputs2['attention_mask'],\n",
    "        \"labels\": label_map[label]\n",
    "    }\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-xsmall')\n",
    "\n",
    "\n",
    "\n",
    "# Encode datasets\n",
    "train_dataset = train_data.map(preprocess_input, fn_kwargs={'tokenizer': tokenizer})\n",
    "val_dataset = val_data.map(preprocess_input, fn_kwargs={'tokenizer': tokenizer})\n",
    "test_dataset = test_data.map(preprocess_input, fn_kwargs={'tokenizer': tokenizer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "class TwinnedModel(nn.Module):\n",
    "    def __init__(self, freeze_weights, model_name1='microsoft/deberta-v3-xsmall', model_name2='microsoft/deberta-v3-xsmall', hidden_size, num_classes=3):\n",
    "        super(TwinnedModel, self).__init__()\n",
    "\n",
    "        # Load pre-trained models\n",
    "        self.model1 = DebertaV2ForSequenceClassification.from_pretrained(model_name1, num_labels=hidden_size)   # here i do not use num_classes because i want to combine features to input to linear layers\n",
    "        self.model2 = DebertaV2ForSequenceClassification.from_pretrained(model_name2, num_labels=hidden_size)\n",
    "\n",
    "        # Freeze the models if fine-tuning is not needed\n",
    "        for param in self.model1.parameters():\n",
    "            param.requires_grad = freeze_weights\n",
    "        for param in self.model2.parameters():\n",
    "            param.requires_grad = freeze_weights\n",
    "\n",
    "        # Define a fully connected layer to combine the hidden states\n",
    "        combined_hidden_size = self.model1.config.hidden_size + self.model2.config.hidden_size\n",
    "        self.fc = nn.Linear(combined_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, labels=None):\n",
    "        # Get outputs from both models (hidden states instead of logits)\n",
    "        outputs1 = self.model1(input_ids1, attention_mask=attention_mask1, output_hidden_states=True)\n",
    "        outputs2 = self.model2(input_ids2, attention_mask=attention_mask2, output_hidden_states=True)\n",
    "\n",
    "        # Get the last hidden state (hidden_states[-1])\n",
    "        hidden_state1 = outputs1.hidden_states[-1][:, 0, :]  # [CLS] token representation\n",
    "        hidden_state2 = outputs2.hidden_states[-1][:, 0, :]  # [CLS] token representation\n",
    "\n",
    "        # Concatenate the hidden states\n",
    "        combined_hidden = torch.cat((hidden_state1, hidden_state2), dim=1)\n",
    "\n",
    "        # Pass the combined hidden states through the fully connected layer\n",
    "        logits = self.fc(combined_hidden)\n",
    "\n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data collator\n",
    "def data_collator(features):\n",
    "    input_ids1 = torch.tensor([f[\"input_ids1\"] for f in features], dtype=torch.long)\n",
    "    attention_mask1 = torch.tensor([f[\"attention_mask1\"] for f in features], dtype=torch.long)\n",
    "    input_ids2 = torch.tensor([f[\"input_ids2\"] for f in features], dtype=torch.long)\n",
    "    attention_mask2 = torch.tensor([f[\"attention_mask2\"] for f in features], dtype=torch.long)\n",
    "    labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"input_ids1\": input_ids1,\n",
    "        \"attention_mask1\": attention_mask1,\n",
    "        \"input_ids2\": input_ids2,\n",
    "        \"attention_mask2\": attention_mask2,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "     per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,  # Simulate batch size of 8*4=32 but without loading all at once\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,     # Train ONCE on the original fever and ONCE on the augmented one\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = TwinnedModel(do_finetuning=FREEZE_BACKBONE, hidden_size=128)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "# trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2287/2287 [02:23<00:00, 15.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on the test set: 0.2832054903064946\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to ./results/confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import os\n",
    "\n",
    "# Ensure the result directory exists\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Compute F1 score\n",
    "f1 = f1_score(labels, preds, average='weighted')\n",
    "print(f\"F1 Score on the test set: {f1}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('./results/confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Confusion matrix saved to ./results/confusion_matrix.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
